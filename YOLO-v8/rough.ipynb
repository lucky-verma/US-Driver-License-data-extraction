{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import easyocr\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "model = YOLO(\"best.pt\")\n",
    "reader = easyocr.Reader(['en'], gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.49  Python-3.7.13 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce RTX 2070 SUPER, 8192MiB)\n",
      "Model summary (fused): 218 layers, 25842655 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\n",
      "0: 512x800 1 date_of_birth, 1 name, 29.3ms\n",
      "Speed: 1.0ms preprocess, 29.3ms inference, 3.5ms postprocess per image at shape (1, 3, 800, 800)\n"
     ]
    }
   ],
   "source": [
    "# from PIL\n",
    "im1 = Image.open(\"../Donut/deployment/0.jpg\")\n",
    "\n",
    "# from list of PIL/ndarray\n",
    "results = model.predict(source=im1, classes=[1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], device='cuda:0') tensor([[169.,  82., 281., 118.]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Boxes' object has no attribute 'name'. See valid attributes below.\n\n    A class for storing and manipulating detection boxes.\n\n    Args:\n        boxes (torch.Tensor) or (numpy.ndarray): A tensor or numpy array containing the detection boxes,\n            with shape (num_boxes, 6). The last two columns should contain confidence and class values.\n        orig_shape (tuple): Original image size, in the format (height, width).\n\n    Attributes:\n        boxes (torch.Tensor) or (numpy.ndarray): A tensor or numpy array containing the detection boxes,\n            with shape (num_boxes, 6).\n        orig_shape (torch.Tensor) or (numpy.ndarray): Original image size, in the format (height, width).\n        is_track (bool): True if the boxes also include track IDs, False otherwise.\n\n    Properties:\n        xyxy (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format.\n        conf (torch.Tensor) or (numpy.ndarray): The confidence values of the boxes.\n        cls (torch.Tensor) or (numpy.ndarray): The class values of the boxes.\n        id (torch.Tensor) or (numpy.ndarray): The track IDs of the boxes (if available).\n        xywh (torch.Tensor) or (numpy.ndarray): The boxes in xywh format.\n        xyxyn (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format normalized by original image size.\n        xywhn (torch.Tensor) or (numpy.ndarray): The boxes in xywh format normalized by original image size.\n        data (torch.Tensor): The raw bboxes tensor\n\n    Methods:\n        cpu(): Move the object to CPU memory.\n        numpy(): Convert the object to a numpy array.\n        cuda(): Move the object to CUDA memory.\n        to(*args, **kwargs): Move the object to the specified device.\n        pandas(): Convert the object to a pandas DataFrame (not yet implemented).\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28320\\2701574567.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# crop each box\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucki\\anaconda3\\envs\\donut\\lib\\site-packages\\ultralytics\\yolo\\engine\\results.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Boxes' object has no attribute 'name'. See valid attributes below.\n\n    A class for storing and manipulating detection boxes.\n\n    Args:\n        boxes (torch.Tensor) or (numpy.ndarray): A tensor or numpy array containing the detection boxes,\n            with shape (num_boxes, 6). The last two columns should contain confidence and class values.\n        orig_shape (tuple): Original image size, in the format (height, width).\n\n    Attributes:\n        boxes (torch.Tensor) or (numpy.ndarray): A tensor or numpy array containing the detection boxes,\n            with shape (num_boxes, 6).\n        orig_shape (torch.Tensor) or (numpy.ndarray): Original image size, in the format (height, width).\n        is_track (bool): True if the boxes also include track IDs, False otherwise.\n\n    Properties:\n        xyxy (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format.\n        conf (torch.Tensor) or (numpy.ndarray): The confidence values of the boxes.\n        cls (torch.Tensor) or (numpy.ndarray): The class values of the boxes.\n        id (torch.Tensor) or (numpy.ndarray): The track IDs of the boxes (if available).\n        xywh (torch.Tensor) or (numpy.ndarray): The boxes in xywh format.\n        xyxyn (torch.Tensor) or (numpy.ndarray): The boxes in xyxy format normalized by original image size.\n        xywhn (torch.Tensor) or (numpy.ndarray): The boxes in xywh format normalized by original image size.\n        data (torch.Tensor): The raw bboxes tensor\n\n    Methods:\n        cpu(): Move the object to CPU memory.\n        numpy(): Convert the object to a numpy array.\n        cuda(): Move the object to CUDA memory.\n        to(*args, **kwargs): Move the object to the specified device.\n        pandas(): Convert the object to a pandas DataFrame (not yet implemented).\n    "
     ]
    }
   ],
   "source": [
    "boxes = results[0].boxes\n",
    "for box in boxes:\n",
    "    print(box.cls, box.xyxy)\n",
    "\n",
    "    print(box.name)\n",
    "\n",
    "    # crop each box \n",
    "    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "    cropped = im1.crop((x1, y1, x2, y2))\n",
    "\n",
    "    plt.imshow(cropped)\n",
    "    plt.show()\n",
    "\n",
    "    print(reader.readtext(np.array(cropped), detail=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{ '_keys': <generator object Results.__init__.<locals>.<genexpr> at 0x0000023E59012CC8>,\n",
       "   'boxes': Ultralytics YOLO Boxes\n",
       " type: <class 'torch.Tensor'>\n",
       " shape: torch.Size([2, 6])\n",
       " dtype: torch.float32\n",
       "  + tensor([[169.00000,  82.00000, 281.00000, 118.00000,   0.87396,   3.00000],\n",
       "         [168.00000, 203.00000, 292.00000, 222.00000,   0.79858,   1.00000]], device='cuda:0'),\n",
       "   'masks': None,\n",
       "   'names': {0: 'address', 1: 'date_of_birth', 2: 'license_number', 3: 'name', 4: 'sex'},\n",
       "   'orig_img': array([[[ 1,  8,  0],\n",
       "         [ 6, 13,  0],\n",
       "         [ 8,  9,  0],\n",
       "         ...,\n",
       "         [14, 15, 13],\n",
       "         [ 3,  4,  2],\n",
       "         [ 2,  3,  1]],\n",
       " \n",
       "        [[ 2,  8,  0],\n",
       "         [ 8, 14,  3],\n",
       "         [ 2,  3,  0],\n",
       "         ...,\n",
       "         [ 0,  1,  0],\n",
       "         [ 2,  3,  1],\n",
       "         [ 2,  3,  1]],\n",
       " \n",
       "        [[ 1,  6,  0],\n",
       "         [ 0,  2,  0],\n",
       "         [ 8,  8,  2],\n",
       "         ...,\n",
       "         [ 2,  3,  1],\n",
       "         [ 1,  1,  1],\n",
       "         [ 1,  1,  1]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0,  1,  0],\n",
       "         [ 3,  8,  7],\n",
       "         [ 0,  0,  0],\n",
       "         ...,\n",
       "         [ 4,  2,  2],\n",
       "         [ 6,  4,  4],\n",
       "         [ 7,  5,  5]],\n",
       " \n",
       "        [[ 3,  8,  7],\n",
       "         [ 0,  1,  0],\n",
       "         [ 0,  0,  0],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 4,  2,  2],\n",
       "         [ 3,  3,  3]],\n",
       " \n",
       "        [[ 3,  7,  8],\n",
       "         [ 0,  0,  1],\n",
       "         [ 0,  5,  4],\n",
       "         ...,\n",
       "         [ 6,  8,  8],\n",
       "         [ 2,  2,  2],\n",
       "         [ 0,  2,  2]]], dtype=uint8),\n",
       "   'orig_shape': (317, 506),\n",
       "   'path': 'image0.jpg',\n",
       "   'probs': None,\n",
       "   'speed': {'inference': 29.26802635192871, 'postprocess': 3.5047531127929688, 'preprocess': 0.9984970092773438}}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "donut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
